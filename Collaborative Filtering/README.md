# Itemâ€Based Collaborative Filtering Recommender

A Spark RDD implementation of an itemâ€based collaborative filtering system on Yelp review data.  
Youâ€™ll first **build** an itemâ€“item similarity model from training reviews, then **predict** user ratings on held-out pairs.

---

## ğŸ“‹ Overview

1. **Preprocessing & Data Structuring**  
   - Read JSON reviews: each record has `user_id`, `business_id`, and `stars`.  
   - Build two helpful groupings:  
     - **Userâ†’[(business, rating)]** for generating co-rated item pairs and later prediction.  
     - **Businessâ†’[rating]** to compute perâ€item averages and global average.

2. **Model Building (`build.py`)**  
   - For each userâ€™s rated businesses, emit every pair of co-rated items along with their two ratings.  
   - Group by **(itemâ‚, itemâ‚‚)** and keep only pairs with at least _m_ common raters.  
   - Compute the **Pearson correlation** over the paired ratings â†’ similarity `sim`.  
   - Output each pair as one JSON line:
     ```json
     {
       "b1": "BUSINESS_ID_1",
       "b2": "BUSINESS_ID_2",
       "sim": 0.1234,
       "num_common": 42
     }
     ```

3. **Rating Prediction (`predict.py`)**  
   - Load the **itemâ€“item model** into memory as a dict  
     `((b1,b2) â†’ (sim, num_common))`.  
   - For each test pair `(user, item)`:  
     1. Fetch all businesses `j` that `user` has rated.  
     2. Look up `(item, j)` or `(j, item)` in the model â†’ `(sim, cnt)`.  
     3. Apply **shrinkage**:  
        ```python
        shrink = cnt / float(cnt + 50)
        w = sim * shrink
        ```  
     4. Keep **only positive** weights, sort by descending `w`, take top _n_ neighbors.  
     5. Compute the weightedâ€centered prediction:
        p_{u,i} = rÌ„áµ¤ + (âˆ‘â±¼ wáµ¢â±¼ Â· (ráµ¤â±¼ âˆ’ rÌ„áµ¤)) / (âˆ‘â±¼ wáµ¢â±¼)

        where rÌ„áµ¤ is the userâ€™s mean rating.  
     6. **Fallbacks**:  
        - If no positiveâ€weight neighbors â†’ return rÌ„áµ¤.  
        - (Optional) If `user` is unseen â†’ return global average (or business average).

4. **Output & Logging**  
   - Both scripts write a simple **time file** (`--time_file`) containing JSON:
     ```json
     { "time": 32.05 }
     ```
   - **build.py** writes the model as a _single_ JSON file at `--model_file`.  
   - **predict.py** writes one JSON line per prediction to `--output_file`:
     ```json
     {
       "user_id": "USER123",
       "business_id": "BUS456",
       "stars": 3.75
     }
     ```

---

## ğŸ”§ Requirements

- **Python 3.9**  
- **Apache Spark 3.2.1** (RDD API only)  
- **pyspark** Python package  
- Java JDK 8+  

Install Spark and then:
```bash
pip install pyspark
```

## ğŸ“‚ Data Layout

Place your files under `data/`:
- `data/train_review.json` â€” 80% of reviews for training
- `data/val_review.json` â€” (user,business) pairs to predict
- `data/val_review_ratings.json` â€” ground truth for validation

## ğŸš€ How to Run

1. **Build the model**
```bash
python build.py \
  --train_file data/train_review.json \
  --model_file HW4.model \
  --time_file  HW4.build.time \
  --m 5
```
- `--m` = minimum common raters (try 3, 5, 7)

2. **Predict ratings**
```bash
python predict.py \
  --train_file   data/train_review.json \
  --test_file    data/val_review.json \
  --model_file   HW4.model \
  --output_file  HW4.val.out \
  --time_file    HW4.predict.time \
  --n 8
```
- `--n` = number of neighbors (try 4, 6, 8, 10)

After running, compare `HW4.val.out` against `val_review_ratings.json` to compute RMSE.

## âš™ï¸ Tuning & Tips

- Positive similarities only: discard `w â‰¤ 0` to avoid pulling predictions away.
- Sort by `w`, not `|w|`.
- Userâ€mean fallback: returning rÌ„áµ¤ when no neighbors often beats business/global averages.
- Hyperparameter grid:
  - m âˆˆ {3, 5, 7}
  - n âˆˆ {4, 6, 8, 10}
  - Shrinkage constant (in cnt/(cnt+K)) with K âˆˆ {20, 50, 100}
 
A small grid search on your validation split will typically push RMSE below 0.90 and RMSEâ‚â‚‚â‚… under 1.30.

## ğŸ“ˆ Summary

This project builds an itemâ€based collaborative filtering recommender using only Spark RDDs and Python standard libraries.
It covers:
- Data structuring (userâ€item & itemâ€user groupings)
- Pairwise coâ€rating extraction
- Pearson similarity with coâ€rating threshold
- Shrinkageâ€weighted neighborhood prediction
- Runtime logging and singleâ€file model output

